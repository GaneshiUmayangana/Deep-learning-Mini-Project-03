{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFMmV954pxoFFb/Rhg+hTE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GaneshiUmayangana/Deep-learning-Mini-Project-03/blob/main/Deep_Learning_Mini_Project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#English to Sinhala Transalation with Transforms\n",
        "\n"
      ],
      "metadata": {
        "id": "oCc8LKU_FQ-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image"
      ],
      "metadata": {
        "id": "XlyTXePVFdC0"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(url ='https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "utiv28oPFf1R",
        "outputId": "ee92a6d0-c17c-4003-83ef-488a272cedd7"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Necessary Library Imports"
      ],
      "metadata": {
        "id": "cTrLD4wBFjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare the Data"
      ],
      "metadata": {
        "id": "C8WylncCFlj3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4a8hSIN2-mG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mount the Google Drive"
      ],
      "metadata": {
        "id": "KDw0k_x4FtMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJp4c8if3HaQ",
        "outputId": "ec8ba561-6705-4feb-a8c2-9315afb9acd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read the data file"
      ],
      "metadata": {
        "id": "auDV_CGBGQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"/content/drive/My Drive/Colab_Data_Files/sin.txt\"\n",
        "with open(text_file) as f:\n",
        "  lines = f.read().split(\"\\n\")[:-1]\n",
        "i = 0\n",
        "for line in lines:\n",
        "  print(line)\n",
        "  i = i + 1\n",
        "  if(i==20):\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKxG68Yj3UAZ",
        "outputId": "1aacd12d-d8de-4abc-8520-b6894416eae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\tයන්න.\n",
            "Go.\tයන්න.\n",
            "Go.\tයන්න.\n",
            "Go.\tයන්න.\n",
            "Hi.\tආයුබෝවන්.\n",
            "Run!\tදුවන්න!\n",
            "Run.\tදුවන්න.\n",
            "Who?\tකවුද?\n",
            "Wow!\tවාව්!\n",
            "Fire!\tගිනි!\n",
            "Fire!\tගිනි!\n",
            "Fire!\tගිනි!\n",
            "Help!\tඋදව්!\n",
            "Help!\tඋදව්!\n",
            "Help!\tඋදව්!\n",
            "Jump!\tපනින්න!\n",
            "Jump.\tපනින්න.\n",
            "Stop!\tනවත්වන්න!\n",
            "Stop!\tනවත්වන්න!\n",
            "Stop!\tනවත්වන්න!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in range(len(lines)-10,len(lines)):\n",
        "  print(lines[x])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKMzkxhB3clq",
        "outputId": "dea175a4-3387-4656-f580-f9702cbf5ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom promised.\tටොම් පොරොන්දු විය.\n",
            "Tom promised.\tටොම් පොරොන්දු විය.\n",
            "Tom ran away.\tටොම් පලා ගියේය.\n",
            "Tom relented.\tටොම් පසුබට විය.\n",
            "Tom relented.\tටොම් පසුබට විය.\n",
            "Tom relented.\tටොම් අනුකම්පා කළා.\n",
            "Tom resigned.\tතෝමස් ඉල්ලා අස්විය.\n",
            "Tom saw Mary.\tටොම් මේරිව දැක්කා.\n",
            "Tom screamed.\tටොම් කෑගැසුවා.\n",
            "Tom shot her.\tටොමස් ඔහුට වෙඩි තැබුවේය.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split the English and Sinhala translation pairs"
      ],
      "metadata": {
        "id": "uIW4St6OGU7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs = []\n",
        "\n",
        "for line in lines:\n",
        "    parts = line.split(\"\\t\")\n",
        "\n",
        "    if len(parts) == 2:\n",
        "        english, sinhala = parts\n",
        "        sinhala = \"[start]\" + sinhala + \"[end]\"\n",
        "        text_pairs.append((english, sinhala))\n",
        "    else:\n",
        "\n",
        "        print(\"Skipping line:\", line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H49viQzg3h_e",
        "outputId": "385a2432-70bd-4a0d-8b34-a120bdceb411"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping line: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "  print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvl07dPkxsTI",
        "outputId": "90c79b39-ffde-4066-f5a8-c50bb9b62101"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I mean it.', '[start]මම බැරෑරුම්.[end]')\n",
            "('Keep running.', '[start]දිගටම දුවන්න.[end]')\n",
            "(\"It's here.\", '[start]මෙතන.[end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Randomize the data"
      ],
      "metadata": {
        "id": "zfn5AMhnGb1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)"
      ],
      "metadata": {
        "id": "0pRoql5G3vlw"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Splitting the data into training, validation and testing"
      ],
      "metadata": {
        "id": "ai0PPLWAGhcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "print(\"Total sentences:\",len(text_pairs))\n",
        "print(\"Training set size:\",len(train_pairs))\n",
        "print(\"Validation set size:\",len(val_pairs))\n",
        "print(\"Testing set size:\",len(test_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClpY46Id338E",
        "outputId": "f37602b0-ccd0-4aed-fabc-e1886970bf33"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total sentences: 3500\n",
            "Training set size: 2450\n",
            "Validation set size: 525\n",
            "Testing set size: 525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_pairs)+len(val_pairs)+len(test_pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTGmlxkb37p2",
        "outputId": "1cfc84fa-e98f-4262-b838-963a6802df53"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3500"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Removing Punctuation"
      ],
      "metadata": {
        "id": "6_lk2qTeGk5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "f\"[{re.escape(strip_chars)}]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tYoL52oM3_EU",
        "outputId": "01ded577-6de7-4e86-eb4e-c6cdb3354b4b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\\\\\\\\\^_`\\\\{\\\\|\\\\}\\\\~¿]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f\"{3+5}\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4NWmsNo34BT8",
        "outputId": "3294c397-2494-488e-e4bc-22b06079f018"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#vectorizing the English and Sinhala text pairs"
      ],
      "metadata": {
        "id": "Mc-0pbPOGnsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "  max_tokens=vocab_size,\n",
        "  output_mode=\"int\",\n",
        "  output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "target_vectorization = layers.TextVectorization(\n",
        "  max_tokens=vocab_size,\n",
        "  output_mode=\"int\",\n",
        "  output_sequence_length=sequence_length + 1,\n",
        "  standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "lTA6rGgh4FzC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing datasets for the translation task"
      ],
      "metadata": {
        "id": "nvQ39t8BGrCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "    \"english\": eng,\n",
        "    \"sinhala\": spa[:, :-1],\n",
        "    }, spa[:, 1:])\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "for inputs, targets in train_ds.take(1):\n",
        "  print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "  print(f\"inputs['sinhala'].shape: {inputs['sinhala'].shape}\")\n",
        "  print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT5tq28h4aeh",
        "outputId": "60c7c9cd-6d7d-4122-a72e-2715923740f1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['sinhala'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(train_ds.as_numpy_iterator())[20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaQgxLc-4qBT",
        "outputId": "132efa7f-fe69-4c7a-bc38-f9c3fdd4a2f2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'english': array([[  3, 709,   0, ...,   0,   0,   0],\n",
            "       [  2,  30, 144, ...,   0,   0,   0],\n",
            "       [175, 815,   0, ...,   0,   0,   0],\n",
            "       ...,\n",
            "       [191, 101,   0, ...,   0,   0,   0],\n",
            "       [  5,  20,   0, ...,   0,   0,   0],\n",
            "       [ 21,  74,   0, ...,   0,   0,   0]]), 'sinhala': array([[   3,  312, 1198, ...,    0,    0,    0],\n",
            "       [   2,  127,    0, ...,    0,    0,    0],\n",
            "       [   2,  161,    0, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [  99,  179,    0, ...,    0,    0,    0],\n",
            "       [   2,  200,    0, ...,    0,    0,    0],\n",
            "       [ 170,   21,    0, ...,    0,    0,    0]])}, array([[ 312, 1198,    0, ...,    0,    0,    0],\n",
            "       [ 127,    0,    0, ...,    0,    0,    0],\n",
            "       [ 161,    0,    0, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 179,    0,    0, ...,    0,    0,    0],\n",
            "       [ 200,    0,    0, ...,    0,    0,    0],\n",
            "       [  21,    0,    0, ...,    0,    0,    0]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer encoder implemented as a subclassed layer"
      ],
      "metadata": {
        "id": "5oLm_Zf2GwFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = layers.MultiHeadAttention(\n",
        "      num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "        layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "  def call(self, inputs, mask=None):\n",
        "    if mask is not None:\n",
        "        mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(\n",
        "        inputs, inputs, attention_mask=mask)\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "      \"embed_dim\": self.embed_dim,\n",
        "      \"num_heads\": self.num_heads,\n",
        "      \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "nkifO00W4uAm"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The transformer decorder"
      ],
      "metadata": {
        "id": "dl1A4PayGy-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = layers.MultiHeadAttention(\n",
        "      num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.attention_2 = layers.MultiHeadAttention(\n",
        "      num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "      [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "      layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "        \"embed_dim\": self.embed_dim,\n",
        "        \"num_heads\": self.num_heads,\n",
        "        \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config\n",
        "\n",
        "  def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1),\n",
        "        tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    return tf.tile(mask, mult)\n",
        "  def call(self, inputs, encoder_outputs, mask=None):\n",
        "    causal_mask = self.get_causal_attention_mask(inputs)\n",
        "    if mask is not None:\n",
        "      padding_mask = tf.cast(\n",
        "          mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "      padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "    else:\n",
        "      padding_mask = mask\n",
        "    attention_output_1 = self.attention_1(\n",
        "        query=inputs,\n",
        "        value=inputs,\n",
        "        key=inputs,\n",
        "        attention_mask=causal_mask)\n",
        "    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "    attention_output_2 = self.attention_2(\n",
        "        query=attention_output_1,\n",
        "        value=encoder_outputs,\n",
        "        key=encoder_outputs,\n",
        "        attention_mask=padding_mask,\n",
        "    )\n",
        "    attention_output_2 = self.layernorm_2(\n",
        "        attention_output_1 + attention_output_2)\n",
        "    proj_output = self.dense_proj(attention_output_2)\n",
        "    return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "1RHnHemd5Pa2"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Positional Encoding"
      ],
      "metadata": {
        "id": "oakTexdqG33O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "  def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(\n",
        "        input_dim=input_dim, output_dim=output_dim)\n",
        "    self.position_embeddings = layers.Embedding(\n",
        "      input_dim=sequence_length, output_dim=output_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1)\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(PositionalEmbedding, self).get_config()\n",
        "    config.update({\n",
        "        \"output_dim\": self.output_dim,\n",
        "        \"sequence_length\": self.sequence_length,\n",
        "        \"input_dim\": self.input_dim,\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "4fEReYrw6ih1"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"sinhala\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "K1h8Xi9n67EY"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUXNjDWA69_4",
        "outputId": "11fabf69-a2bb-4dd4-df7c-7e054f7219ce"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " english (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " sinhala (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " positional_embedding_4 (Po  (None, None, 256)            3845120   ['english[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " positional_embedding_5 (Po  (None, None, 256)            3845120   ['sinhala[0][0]']             \n",
            " sitionalEmbedding)                                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder_2 (Tra  (None, None, 256)            3155456   ['positional_embedding_4[0][0]\n",
            " nsformerEncoder)                                                   ']                            \n",
            "                                                                                                  \n",
            " transformer_decoder_2 (Tra  (None, None, 256)            5259520   ['positional_embedding_5[0][0]\n",
            " nsformerDecoder)                                                   ',                            \n",
            "                                                                     'transformer_encoder_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, None, 256)            0         ['transformer_decoder_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_14 (Dense)            (None, None, 15000)          3855000   ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19960216 (76.14 MB)\n",
            "Trainable params: 19960216 (76.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the sequence-to-sequence Transformer"
      ],
      "metadata": {
        "id": "XpPfsZjTG8o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjxDOY4z7Cjw",
        "outputId": "c2df05b0-1087-4038-ffb0-f468b1cb28c6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "39/39 [==============================] - 174s 4s/step - loss: 4.3745 - accuracy: 0.4222 - val_loss: 4.2484 - val_accuracy: 0.4711\n",
            "Epoch 2/30\n",
            "39/39 [==============================] - 122s 3s/step - loss: 3.7180 - accuracy: 0.4956 - val_loss: 4.0544 - val_accuracy: 0.4921\n",
            "Epoch 3/30\n",
            "39/39 [==============================] - 123s 3s/step - loss: 3.3581 - accuracy: 0.5325 - val_loss: 3.8500 - val_accuracy: 0.5101\n",
            "Epoch 4/30\n",
            "39/39 [==============================] - 123s 3s/step - loss: 2.9612 - accuracy: 0.5784 - val_loss: 3.7001 - val_accuracy: 0.5260\n",
            "Epoch 5/30\n",
            "39/39 [==============================] - 124s 3s/step - loss: 2.6708 - accuracy: 0.6098 - val_loss: 3.6299 - val_accuracy: 0.5346\n",
            "Epoch 6/30\n",
            "39/39 [==============================] - 123s 3s/step - loss: 2.7585 - accuracy: 0.5970 - val_loss: 3.5863 - val_accuracy: 0.5317\n",
            "Epoch 7/30\n",
            "39/39 [==============================] - 124s 3s/step - loss: 2.2271 - accuracy: 0.6627 - val_loss: 3.5316 - val_accuracy: 0.5339\n",
            "Epoch 8/30\n",
            "39/39 [==============================] - 124s 3s/step - loss: 2.0310 - accuracy: 0.6879 - val_loss: 3.4591 - val_accuracy: 0.5556\n",
            "Epoch 9/30\n",
            "39/39 [==============================] - 124s 3s/step - loss: 1.8462 - accuracy: 0.7093 - val_loss: 3.4409 - val_accuracy: 0.5462\n",
            "Epoch 10/30\n",
            "39/39 [==============================] - 122s 3s/step - loss: 1.6696 - accuracy: 0.7319 - val_loss: 3.3930 - val_accuracy: 0.5664\n",
            "Epoch 11/30\n",
            "39/39 [==============================] - 126s 3s/step - loss: 1.5139 - accuracy: 0.7527 - val_loss: 3.3426 - val_accuracy: 0.5758\n",
            "Epoch 12/30\n",
            "39/39 [==============================] - 123s 3s/step - loss: 1.3561 - accuracy: 0.7762 - val_loss: 3.3528 - val_accuracy: 0.5649\n",
            "Epoch 13/30\n",
            "39/39 [==============================] - 123s 3s/step - loss: 1.2162 - accuracy: 0.7946 - val_loss: 3.2904 - val_accuracy: 0.5866\n",
            "Epoch 14/30\n",
            "39/39 [==============================] - 124s 3s/step - loss: 1.1005 - accuracy: 0.8205 - val_loss: 3.2905 - val_accuracy: 0.5895\n",
            "Epoch 15/30\n",
            "39/39 [==============================] - 124s 3s/step - loss: 1.7233 - accuracy: 0.7728 - val_loss: 3.3016 - val_accuracy: 0.5938\n",
            "Epoch 16/30\n",
            "39/39 [==============================] - 122s 3s/step - loss: 0.8847 - accuracy: 0.8502 - val_loss: 3.3739 - val_accuracy: 0.5772\n",
            "Epoch 17/30\n",
            "39/39 [==============================] - 124s 3s/step - loss: 0.8033 - accuracy: 0.8699 - val_loss: 3.2870 - val_accuracy: 0.6046\n",
            "Epoch 18/30\n",
            "39/39 [==============================] - 123s 3s/step - loss: 0.7303 - accuracy: 0.8792 - val_loss: 3.2457 - val_accuracy: 0.6104\n",
            "Epoch 19/30\n",
            "39/39 [==============================] - 125s 3s/step - loss: 0.6421 - accuracy: 0.8926 - val_loss: 3.2505 - val_accuracy: 0.6053\n",
            "Epoch 20/30\n",
            "39/39 [==============================] - 122s 3s/step - loss: 0.5770 - accuracy: 0.9051 - val_loss: 3.2218 - val_accuracy: 0.6133\n",
            "Epoch 21/30\n",
            "39/39 [==============================] - 123s 3s/step - loss: 0.4968 - accuracy: 0.9189 - val_loss: 3.2601 - val_accuracy: 0.6126\n",
            "Epoch 22/30\n",
            "39/39 [==============================] - 125s 3s/step - loss: 0.4487 - accuracy: 0.9240 - val_loss: 3.2757 - val_accuracy: 0.6176\n",
            "Epoch 23/30\n",
            "39/39 [==============================] - 123s 3s/step - loss: 0.3931 - accuracy: 0.9360 - val_loss: 3.2311 - val_accuracy: 0.6169\n",
            "Epoch 24/30\n",
            "39/39 [==============================] - 124s 3s/step - loss: 0.3598 - accuracy: 0.9440 - val_loss: 3.3639 - val_accuracy: 0.5938\n",
            "Epoch 25/30\n",
            "39/39 [==============================] - 123s 3s/step - loss: 0.3082 - accuracy: 0.9551 - val_loss: 3.2777 - val_accuracy: 0.6205\n",
            "Epoch 26/30\n",
            "39/39 [==============================] - 122s 3s/step - loss: 0.2799 - accuracy: 0.9582 - val_loss: 3.2938 - val_accuracy: 0.6241\n",
            "Epoch 27/30\n",
            "39/39 [==============================] - 124s 3s/step - loss: 0.2555 - accuracy: 0.9623 - val_loss: 3.3213 - val_accuracy: 0.6097\n",
            "Epoch 28/30\n",
            "39/39 [==============================] - 124s 3s/step - loss: 0.2321 - accuracy: 0.9649 - val_loss: 3.2719 - val_accuracy: 0.6241\n",
            "Epoch 29/30\n",
            "39/39 [==============================] - 125s 3s/step - loss: 0.2049 - accuracy: 0.9714 - val_loss: 3.4421 - val_accuracy: 0.6118\n",
            "Epoch 30/30\n",
            "39/39 [==============================] - 135s 3s/step - loss: 0.1960 - accuracy: 0.9691 - val_loss: 3.4444 - val_accuracy: 0.6140\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e3beb676b90>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  decoded_sentence =\"[start]\"\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization(\n",
        "        [decoded_sentence])[:,:-1]\n",
        "    predictions = transformer(\n",
        "        [tokenized_input_sentence,tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(predictions[0,i,:])\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        "  return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPyzR4UT7N0M",
        "outputId": "db3aed11-4cce-433c-a911-195e8431b84b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "I saw a dog.\n",
            "[start] බල්ලෙක් දැක්කා[end]     දැක්කා[end]   දැක්කා[end]          \n",
            "-\n",
            "Are you lost?\n",
            "[start] අතරමං වී තිබේද[end]           කළා[end]      \n",
            "-\n",
            "Stay away.\n",
            "[start] ඉන්න[end]                   \n",
            "-\n",
            "I tried.\n",
            "[start] කළා[end]  කළා[end]       කළා[end]          \n",
            "-\n",
            "Are you bald?\n",
            "[start] තට්ටය[end]         තට්ටය[end]          \n",
            "-\n",
            "Go find out.\n",
            "[start] යන්න[end]                   \n",
            "-\n",
            "Keep them.\n",
            "[start] save කරන්න[end]                  \n",
            "-\n",
            "I screamed.\n",
            "[start] කෑ ගැහුවා[end]        කළා[end]          \n",
            "-\n",
            "Look at us.\n",
            "[start] දිහා බලන්න[end]                  \n",
            "-\n",
            "We waited.\n",
            "[start] බලා සිටියේය[end]                  \n",
            "-\n",
            "Come forward.\n",
            "[start] එන්න[end]                   \n",
            "-\n",
            "Hold this.\n",
            "[start] සුවඳ බලන්න[end]                  \n",
            "-\n",
            "I like pizza.\n",
            "[start] ඒකට කැමතියි[end]        කැමතියි[end]          \n",
            "-\n",
            "I like opera.\n",
            "[start] ඒකට කැමති වුණා[end]                 \n",
            "-\n",
            "That's my CD.\n",
            "[start] මගේ සීඩී එක[end]       ඒක කළා[end]         \n",
            "-\n",
            "I talked.\n",
            "[start] කතා කළා[end]        කළා[end]          \n",
            "-\n",
            "We can go.\n",
            "[start] යා යුතුයි[end]        කළා[end]          \n",
            "-\n",
            "Are you Tom?\n",
            "[start] වේ[end]                   \n",
            "-\n",
            "Can I stay?\n",
            "[start] බලා සිටිය හැකිද[end]                 \n",
            "-\n",
            "You're fine.\n",
            "[start] හොඳයි ද[end]                  \n"
          ]
        }
      ]
    }
  ]
}